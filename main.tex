\documentclass[12pt,letterpaper]{article}
% the package for network diagram
\usepackage{tikz-network}
% the package for the Axis plot
\usetikzlibrary{shapes}
\usepackage{pgfplots}
% the package for fill color in Axis Plot
\usepgfplotslibrary{fillbetween}
% Matlab code package
\usepackage[framed,numbered,autolinebreaks,useliterate]{mcode}

\usepackage[english]{babel} 
\usepackage[utf8]{inputenc}	
\spanishdecimal{.}
% \usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{amsmath,amsfonts,amssymb,amscd,amsthm}

%Theorem Package
\newtheorem{theorem}{Theorem}
\usepackage{multirow,booktabs}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{calc}
\usepackage{multicol}
\usepackage{cancel}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage[margin=2.5cm]{geometry}
\usepackage{floatrow}
\newlength{\tabcont}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
\title{TALLER ALGEBRA LINEAL ISAAC RODRIGUEZ}
\newcommand\course{Soft Computing ENGG6570}	
\newcommand\semester{2021} 
\newcommand\yourname{Yaowen Mei (1177855)}  
\theoremstyle{definition}
\newtheorem{defn}{Definición}
\newtheorem{reg}{Regla}
\newtheorem{ejer}{EJERCICIO}
\pagestyle{fancyplain}
\headheight 32pt
\lhead{\yourname\ \vspace{0.05cm} \\ \course}
\chead{\textbf{\Large Homework-3}}
\rhead{2021/06/09}

%% QED in black square
\renewcommand{\qedsymbol}{\hfill\blacksquare}
\newcommand{\qedwhite}{\hfill \ensuremath{\Box}}
%% Proof and Lamma
\newtheorem{lemma}[theorem]{Lemma}

\usepackage[UTF8]{ctex} % 中文宏包
\usepackage{mhchem} %化学式宏包
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{mwe}

\begin{document}
% \include{homework1}
% \include{homework2}
% \include{homeworktest3}

\textbf{Question-1}: For a neural network for robot path planning, the neural dynamics of each
neuron is described by an additive equation,
\begin{equation}
    \frac{{d{x_i}}}{{dt}} =  - A{x_i} + \sum\limits_{j = 1,j \ne i}^n {{w_{ij}}f\left( {{x_j}} \right) + {I_i} \label{eq1st}}
\end{equation}

where $w_{ij} = w_{ji}$; $f(a) = \max \left\{ {a,0} \right\}$ is a linear-above threshold function; and $I_i$ is the external
input from the environment. Prove this neural network system is stable using the Lyapunov
stability theory.

\textbf{Solution-1}: Recall the Grossberg's general model's :
\begin{equation}
    \frac{{d{x_i}}}{{dt}} = {a_i}\left( {{x_i}} \right)\left[ {{b_i}\left( {{x_i}} \right) - \sum\limits_{j = 1,j \ne i}^n {{C_{ij}}d_j\left( {{x_j}} \right)} } \right] \label{eq1grossberg}
\end{equation}

Where: $a_i$ is the amplification function for neuron i; $b_i$ is the self-signal function for neuron i; $d_j$ is the other-signal function (activation function of other neuron j), $c_{ij}$ is the connection weight from neuron j to neuron i.
\\
\\
Let's try to re-write the original additive equation in Grossberg's general model's format. Note that for all the equations below, we will assume the principal  diagonal elements in matrix $w$ and $C$ are all zero so that we can ignore the term $i \ne j$ from the summation.
\begin{equation}
    \frac{{d{x_i}}}{{dt}} = \left[ {\left( { - A{x_i} + {I_i}} \right) - \sum\limits_{j = 1}^n {\left( { - {w_{ij}}} \right)f\left( {{x_j}} \right)} } \right] 
    \label{eq2rewrite}
\end{equation}
By comparing equation \ref{eq2rewrite} with equation \ref{eq1grossberg}, one can proof the additive equation's stability by showing  all the 3 stability criteria are satisfied accroding to Grossberg version's Lyapunov stability theorem:
 \[{C_{ij}} = {-w_{ij}} \equiv {-w_{ji}} = {C_{ji}} \textrm{ \qquad  (Proof for symmetry)}\]
 \[ {a_i}\left( {{x_i}} \right) = 1 > 0 \textrm{  \qquad    (Proof for positivity)}\]
\[d_j(x_j) = f(x_j)=\max \left\{ a, 0\right\} \ge 0  \textrm{   \qquad    (Proof for monotonicity)}\qed\]
\\\clearpage
\textbf{Question #2} A neural network is characterized by an input-output equation,
\begin{equation}
    {y_i}\left( {t + 1} \right) = \phi \left( {\sum\limits_{j = 1}^n {{w_{ij}}{y_j}\left( t \right) + {\theta _i}} } \right)
\end{equation}
Where \textcolor{red} {$w_{ij} = w_{ji}$, and  $\phi (a) = ({1 + {e^{ - {a}}}})^{-1}$ is the sigmoid function}, $\theta_i$ is the threshold. (1) Use the function-summation exchange to transform this equation into an additive equation; (2) Prove the stability of this system.

\textbf{Solution #2----Step-1:} transform this equation into an additive equation:
\begin{equation}
{{\dot y}_i} = \frac{{\Delta {y_i}}}{{\Delta t}} = \frac{{{y_i}\left( {t + 1} \right) - {y_i}\left( t \right)}}{1} =  - {y_i}\left( t \right) + \phi \left( {{x_i}} \right) \label{eq4dif}
\end{equation}
Where $a_i$ is defined as:
\begin{equation}
    {x_i} \equiv \sum\limits_{j = 1}^n {{w_{ij}}{y_j}\left( t \right) + {\theta _i}} \label{eq5dif}
\end{equation}

Multiply $\sum\limits_{i = 1}^n {{w_{ji}}}$ on each side of equation \ref{eq4dif}:

\begin{equation}
    \sum\limits_{i = 1}^n {{w_{ji}}} {{\dot y}_i} =  - \sum\limits_{i = 1}^n {{w_{ji}}} {y_i}\left( t \right) + \sum\limits_{i = 1}^n {{w_{ji}}} \phi \left( x_i \right) \label{eq6mul}
\end{equation}

From equation \ref{eq5dif}, the definition of $a_i$, one can write:
\begin{equation}
    {{\dot x}_i} = \frac{{d\left( {\sum\limits_{j = 1}^n {{w_{ij}}{y_j}\left( t \right)} } \right)}}{{dt}} = \sum\limits_{j = 1}^n {{w_{ij}}{{\dot y}_j}\left( t \right)} \label{difa}
\end{equation}

Put equation \ref{difa} and equation \ref{eq5dif} into equation \ref{eq6mul}:
\begin{equation}
    {{\dot x}_j} = {x_j} + {\theta _j} + \sum\limits_{i = 1}^n {{w_{ji}}} \phi \left( {{x_i}} \right)
\end{equation}
Finally, switch i and j, we can write equation in the additive equation format:
\begin{equation}
{{\dot x}_i} = {x_i} + \sum\limits_{j = 1}^n {{w_{ij}}} \phi \left( {{x_j}} \right) + {\theta _i} \label{question2setp1eq}
\end{equation}



\clearpage


\textbf{Solution 2----Step-2:} Note that equation \ref{question2setp1eq} is identical with equation \ref{eq1st} in Question 1, except for:
\[A=-1\]
\[I_i = \theta_i\]
\[f(x_j) \textrm{ is replaced by } \phi(x_j)\] 

Equation \ref{question2setp1eq} can be re-write as:
\begin{equation}
    \frac{{d{x_i}}}{{dt}} = \left[ {\left( { {x_i} + {\theta} \right) - \sum\limits_{j = 1}^n {\left( { - {w_{ij}}} \right)\phi\left( {{x_j}} \right)} } \right] 
    \label{question2eq2rewrite}
\end{equation}
Comparing equation \ref{question2eq2rewrite} with equation \ref{eq1grossberg},
one can proof the additive equation's stability by showing  all the 3 stability criteria are satisfied according to Grossberg version's Lyapunov stability theorem:
 \[{C_{ij}} = {-w_{ij}} \equiv {-w_{ji}} = {C_{ji}} \textrm{ \qquad  (Proof for symmetry)}\]
 \[ {a_i}\left( {{x_i}} \right) = 1 > 0 \textrm{  \qquad    (Proof for positivity)}\]
\[{d_j}({x_j}) = \phi ({x_j}) = {\raise0.7ex\hbox{$1$} \!\mathord{\left/
 {\vphantom {1 {1 + {e^{ - {x_j}}}}}}\right.\kern-\nulldelimiterspace}
\!\lower0.7ex\hbox{${1 + {e^{ - {x_j}}}}$}} \ge 0  \textrm{   \qquad    (Proof for monotonicity)}\qed\]
\end{document}